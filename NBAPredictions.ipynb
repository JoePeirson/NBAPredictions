{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import datetime\n",
    "from twilio.rest import Client\n",
    "from fuzzywuzzy import fuzz\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred).round(3)\n",
    "    precision = precision_score(y_test, y_pred).round(3)\n",
    "    recall = recall_score(y_test, y_pred).round(3)\n",
    "    f1 = f1_score(y_test, y_pred).round(3)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "\n",
    "def transform_poly(df_transform):\n",
    "    poly = PolynomialFeatures(2)\n",
    "    df_poly = pd.DataFrame(poly.fit_transform(df_transform))\n",
    "    df_poly.columns = poly.get_feature_names(df_transform.columns)\n",
    "    df_transform = df_poly.copy()\n",
    "    return df_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 1: Understanding and Contextualising the Problem\n",
    " The first step in a Machine Learning process is to clearly understand the problem and work out how it can be solved in a well-defined and measurable way. We need to determine whether it's a regression, classification, clustering, or other type of problem. <br><br>\n",
    "The objective is to build a model that is able to predict the winners for the fixtures on the 25th of February, 2023. At first glance, this seems like a classification problem, as we want to attribute a probalitic chance to each team winning their particular match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Collect and preprocess the data, carry out EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_box = pd.read_csv('box_scores.csv')\n",
    "df_fixt = pd.read_csv('fixture_information.csv')\n",
    "df_fixt_test = pd.read_csv('test_fixtures.csv')\n",
    "df_fixt_actual = pd.read_csv('test_fixtures_actuals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13322, 15), (6661, 8), (130, 7), (260, 18))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_box.shape,df_fixt.shape,df_fixt_test.shape,df_fixt_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_box['PTS'] = df_box['2PM']*2+df_box['3PM']*3+df_box['FTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_box,df_fixt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_25_feb = pd.merge(df_fixt_actual,df_fixt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to add the train and test dataset together so it is easier to format and feature engineer\n",
    "# I will then split them up before modelling starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['data_type'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_25_feb['data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test_25_feb],ignore_index=True)\n",
    "df_all = df_all.drop(['Attendance','TeamHandicap','IsWinner'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently each team has no defensive stats, by reformating we can easily add def stats\n",
    "\n",
    "df_all['Home'] = df_all['FixtureKey'].apply(lambda x: x.split(' v ')[0])\n",
    "df_all['Away'] = df_all['FixtureKey'].apply(lambda x: x.split(' v ')[1].rsplit(' ',1)[0])\n",
    "df_all['Date'] = df_all['FixtureKey'].apply(lambda x: pd.to_datetime(x.rsplit(' ',1)[1]))\n",
    "df_all['Team_name'] = df_all.apply(lambda x: x['Home'] if x['Team']==1 else x['Away'],axis=1)\n",
    "\n",
    "df_all_def = df_all.copy()\n",
    "df_all_def['Team'] = df_all_def['Team'].apply(lambda x: 2 if x==1 else 1)\n",
    "df_all_def = df_all_def[['FixtureKey', 'Team', '2PM', '2PA', '3PM', '3PA', 'FTM', 'FTA', 'ORB',\n",
    "       'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF','PTS']]\n",
    "for col in df_all_def.columns[2:]:\n",
    "    df_all_def = df_all_def.rename(columns={col:col+'_def'})\n",
    "\n",
    "df_all = pd.merge(df_all,df_all_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the goal in basketball is to generate as big a point difference as possible, I thought it made \n",
    "# more sense to have the att/def dif, rather than two variables for att/def\n",
    "for col in ['2PM', '2PA', '3PM', '3PA', 'FTM', 'FTA', 'ORB',\n",
    "       'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF','PTS']:\n",
    "    df_all[col+'_dif'] = df_all[col]-df_all[col+'_def']\n",
    "\n",
    "df_all = df_all.drop(['2PM', '2PA', '3PM', '3PA', 'FTM', 'FTA', 'ORB',\n",
    "       'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF','PTS','2PM_def',\n",
    "       '2PA_def', '3PM_def', '3PA_def', 'FTM_def', 'FTA_def', 'ORB_def',\n",
    "       'DRB_def', 'AST_def', 'STL_def', 'BLK_def', 'TOV_def', 'PF_def','PTS_def'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FixtureKey</th>\n",
       "      <th>GameType</th>\n",
       "      <th>IsNeutralSite</th>\n",
       "      <th>Season</th>\n",
       "      <th>Team</th>\n",
       "      <th>Team1Conference</th>\n",
       "      <th>Team2Conference</th>\n",
       "      <th>TipOff</th>\n",
       "      <th>data_type</th>\n",
       "      <th>Home</th>\n",
       "      <th>Away</th>\n",
       "      <th>Date</th>\n",
       "      <th>Team_name</th>\n",
       "      <th>2PM_dif</th>\n",
       "      <th>2PA_dif</th>\n",
       "      <th>3PM_dif</th>\n",
       "      <th>3PA_dif</th>\n",
       "      <th>FTM_dif</th>\n",
       "      <th>FTA_dif</th>\n",
       "      <th>ORB_dif</th>\n",
       "      <th>DRB_dif</th>\n",
       "      <th>AST_dif</th>\n",
       "      <th>STL_dif</th>\n",
       "      <th>BLK_dif</th>\n",
       "      <th>TOV_dif</th>\n",
       "      <th>PF_dif</th>\n",
       "      <th>PTS_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LIPSCO v A PEAY 14-Jan-2023</td>\n",
       "      <td>RegularSeason</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>A-Sun</td>\n",
       "      <td>A-Sun</td>\n",
       "      <td>17:00</td>\n",
       "      <td>train</td>\n",
       "      <td>LIPSCO</td>\n",
       "      <td>A PEAY</td>\n",
       "      <td>2023-01-14</td>\n",
       "      <td>A PEAY</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-9</td>\n",
       "      <td>-13</td>\n",
       "      <td>5</td>\n",
       "      <td>-6</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LIPSCO v A PEAY 14-Jan-2023</td>\n",
       "      <td>RegularSeason</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>A-Sun</td>\n",
       "      <td>A-Sun</td>\n",
       "      <td>17:00</td>\n",
       "      <td>train</td>\n",
       "      <td>LIPSCO</td>\n",
       "      <td>A PEAY</td>\n",
       "      <td>2023-01-14</td>\n",
       "      <td>LIPSCO</td>\n",
       "      <td>-2</td>\n",
       "      <td>-12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>-5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    FixtureKey       GameType  IsNeutralSite  Season  Team  \\\n",
       "0  LIPSCO v A PEAY 14-Jan-2023  RegularSeason              0    2023     2   \n",
       "1  LIPSCO v A PEAY 14-Jan-2023  RegularSeason              0    2023     1   \n",
       "\n",
       "  Team1Conference Team2Conference TipOff data_type    Home    Away       Date  \\\n",
       "0           A-Sun           A-Sun  17:00     train  LIPSCO  A PEAY 2023-01-14   \n",
       "1           A-Sun           A-Sun  17:00     train  LIPSCO  A PEAY 2023-01-14   \n",
       "\n",
       "  Team_name  2PM_dif  2PA_dif  3PM_dif  3PA_dif  FTM_dif  FTA_dif  ORB_dif  \\\n",
       "0    A PEAY        2       12       -3       -1       -9      -13        5   \n",
       "1    LIPSCO       -2      -12        3        1        9       13       -5   \n",
       "\n",
       "   DRB_dif  AST_dif  STL_dif  BLK_dif  TOV_dif  PF_dif  PTS_dif  \n",
       "0       -6       -4        2        2       -1       3      -14  \n",
       "1        6        4       -2       -2        1      -3       14  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(2)\n",
    "# each row currently shows ingame data which we would not have if we are betting.\n",
    "# therefore we need to reformat, so each match shows past data for the upcoming match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_data(team_name,date,games_back):\n",
    "    prev_team_games = df_all.loc[(df_all['Team_name']==team_name) & (df_all['Date']<date)].sort_values('Date')\n",
    "    prev_team_games = prev_team_games.tail(games_back)[[\n",
    "          '2PM_dif', '2PA_dif', '3PM_dif', '3PA_dif',\n",
    "       'FTM_dif', 'FTA_dif', 'ORB_dif', 'DRB_dif', 'AST_dif', 'STL_dif',\n",
    "       'BLK_dif', 'TOV_dif', 'PF_dif','PTS_dif']]\n",
    "    for col in prev_team_games.columns:\n",
    "        prev_team_games = prev_team_games.rename(columns={col:col+'_'+str(games_back)})\n",
    "    prev_team_games = prev_team_games.mean().to_frame().T\n",
    "    return prev_team_games.loc[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2025-11-14 11:56:21.976006\n",
      "1000 2025-11-14 11:56:32.578548\n",
      "2000 2025-11-14 11:56:43.241659\n",
      "3000 2025-11-14 11:56:53.605736\n",
      "4000 2025-11-14 11:57:03.724052\n",
      "5000 2025-11-14 11:57:13.813235\n",
      "6000 2025-11-14 11:57:24.105859\n",
      "7000 2025-11-14 11:57:34.983569\n",
      "8000 2025-11-14 11:57:45.562698\n",
      "9000 2025-11-14 11:57:56.005478\n",
      "10000 2025-11-14 11:58:06.511543\n",
      "11000 2025-11-14 11:58:17.136662\n",
      "12000 2025-11-14 11:58:29.894047\n",
      "13000 2025-11-14 11:58:48.160605\n"
     ]
    }
   ],
   "source": [
    "df_all_past_list = []\n",
    "\n",
    "for x in df_all.index:\n",
    "    if x%1000==0: print(x,datetime.now())\n",
    "    df_all_past_list.append(get_past_data(df_all['Team_name'][x],df_all['Date'][x],20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_past = pd.DataFrame(df_all_past_list,columns=['2PM_dif_10', '2PA_dif_10', '3PM_dif_10', '3PA_dif_10',\n",
    "       'FTM_dif_10', 'FTA_dif_10', 'ORB_dif_10', 'DRB_dif_10', 'AST_dif_10', 'STL_dif_10',\n",
    "       'BLK_dif_10', 'TOV_dif_10', 'PF_dif_10','PTS_dif_10'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, df_all_past], axis=1)\n",
    "# now each row includes the teams averages over the last 5 games\n",
    "# the earliest matches in the dataset obviously will not have past_matches so I have removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Won'] = (df_all['PTS_dif']>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_cols = ['2PM_dif', '2PA_dif', '3PM_dif', '3PA_dif',\n",
    "       'FTM_dif', 'FTA_dif', 'ORB_dif', 'DRB_dif', 'AST_dif', 'STL_dif',\n",
    "       'BLK_dif', 'TOV_dif', 'PF_dif','PTS_dif']\n",
    "df_all = df_all.drop(rem_cols,axis=1)\n",
    "#  remove columns that we wouldn't know before the start of the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_opp = df_all.copy()\n",
    "df_all_opp['Team'] = df_all_opp['Team'].apply(lambda x: 2 if x==1 else 1)\n",
    "df_all_opp = df_all_opp[['FixtureKey', 'Team', '2PM_dif_10', '2PA_dif_10', '3PM_dif_10',\n",
    "       '3PA_dif_10', 'FTM_dif_10', 'FTA_dif_10', 'ORB_dif_10', 'DRB_dif_10',\n",
    "       'AST_dif_10', 'STL_dif_10', 'BLK_dif_10', 'TOV_dif_10', 'PF_dif_10',\n",
    "       'PTS_dif_10']]\n",
    "for col in df_all_opp.columns[2:]:\n",
    "    df_all_opp = df_all_opp.rename(columns={col:col+'_opp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all,df_all_opp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "Certain features will be in text form rather than numerical form, therefore we need to reformat those variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_cols = ['TipOff', 'GameType', 'data_type', 'Home', 'Away', 'Team_name']\n",
    "df_all = df_all.drop(rem_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.get_dummies(df_all, columns=['Team1Conference','Team2Conference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_1 = df_all.loc[df_all['Date']>='2023-02-25']\n",
    "df_train = df_all.loc[df_all['Date']<'2023-02-25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_25_feb_final = pd.merge(df_test_1,df_test_25_feb[['FixtureKey','Team','IsWinner']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_25_feb_final = df_test_25_feb_final.loc[df_test_25_feb_final['Team']==1]\n",
    "df_train = df_train.loc[df_train['Team']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_25_feb_final = df_test_25_feb_final.drop(['FixtureKey','Date','Team'],axis=1)\n",
    "df_train = df_train.drop(['FixtureKey','Date','Team'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as each game takes up two rows, and each row has all the features of both home/away I am deleting\n",
    "# one of the duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exploratory Data Analysis (EDA)\n",
    "EDA is the process of trying to gain a better understanding of the data, by identifying missing values, outliers, and understanding the relationships between variables. \n",
    "There are a few steps we can take here:\n",
    "- Graphs: We can use scatter, boxplots, histograms to visually understand the data better\n",
    "- Grouping of data: This helps us see how data is grouped in a table format\n",
    "- Correlation Matrix: Correlation is a statistical metric for measuring to what extent different variables are interdependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbed272b350>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.despine(f, left=True, bottom=True)\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_train,\n",
    "    x=\"Won\",\n",
    "    y=\"2PM_dif_10\",\n",
    "    ci=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Split the Data\n",
    "Now we need to split the dataset into training, validation, and testing sets. The training set is used for model training, the validation set for hyperparameter tuning, and the testing set for evaluating the final model. The typical split is 70-80% for training, 10-15% for validation, and 10-15% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test_25_feb_final.copy()\n",
    "X_final = df_test_final[features]\n",
    "y_final = df_test_final[[target_variable]]\n",
    "\n",
    "target_variable = 'Won'\n",
    "features = df_train.drop([target_variable],axis=1).columns\n",
    "\n",
    "X = df_train[features]\n",
    "y = df_train[[target_variable]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shapes: (5135, 94) (1284, 94)\n",
      "y shapes: (5135, 1) (1284, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X shapes:',X_train.shape, X_test.shape)\n",
    "print('y shapes:',y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature engineering\n",
    "Next we need to carry out feature selection to identify the relevant features. This can be done by reviewing the EDA, knowledge of the topic or testing for feature importance. \n",
    "After that we will then carry out feature transformation, such as applying polynomial features, normalisaiton and/or generating new features from our existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Polynomial Features\n",
    "Polynomial Fetaures can be useful when the relationship between the features and the target variable is nonlinear. \n",
    "By adding Polynomial Fetaures, we can capture more complex patterns and improve the model's ability to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shapes: (5135, 4560) (1284, 4560)\n",
      "y shapes: (5135, 1) (1284, 1)\n",
      "test_25_feb shape (130, 4560)\n"
     ]
    }
   ],
   "source": [
    "X_train = transform_poly(X_train)\n",
    "X_test = transform_poly(X_test)\n",
    "X_final = transform_poly(X_final)\n",
    "print('X shapes:',X_train.shape, X_test.shape)\n",
    "print('y shapes:',y_train.shape, y_test.shape)\n",
    "print('test_25_feb shape',X_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data/Normalisation\n",
    "Normalization is the process of scaling down data. Below I have used MinMaxScaler which scales the data between 0 and 1. This makes the training process less sensitive to the scale of the features. This can result in getting better coefficients after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_final = scaler.transform(X_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Selection and Model Training\n",
    "In this step, We will go through different models trying to find the model the produces the best results. Once we have selected a model we can then fine tune it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.609\n",
      "Precision:  0.697\n",
      "Recall:  0.671\n",
      "F1 Score:  0.684\n"
     ]
    }
   ],
   "source": [
    "lr_class = LogisticRegression()\n",
    "lr_class.fit(X_train, y_train)\n",
    "y_pred = lr_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.589\n",
      "Precision:  0.763\n",
      "Recall:  0.504\n",
      "F1 Score:  0.607\n"
     ]
    }
   ],
   "source": [
    "knn_class = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_class.fit(X_train, y_train)\n",
    "y_pred = knn_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.61\n",
      "Precision:  0.698\n",
      "Recall:  0.67\n",
      "F1 Score:  0.684\n"
     ]
    }
   ],
   "source": [
    "dt_class = DecisionTreeClassifier()\n",
    "dt_class.fit(X_train, y_train)\n",
    "y_pred = dt_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.633\n",
      "Precision:  0.71\n",
      "Recall:  0.706\n",
      "F1 Score:  0.708\n"
     ]
    }
   ],
   "source": [
    "rf_class = RandomForestClassifier()\n",
    "rf_class.fit(X_train, y_train)\n",
    "y_pred = rf_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.451\n",
      "Precision:  0.81\n",
      "Recall:  0.168\n",
      "F1 Score:  0.278\n"
     ]
    }
   ],
   "source": [
    "nb_class = GaussianNB()\n",
    "nb_class.fit(X_train, y_train)\n",
    "y_pred = nb_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(neurons=64, dropout=0.2, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(neurons//2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.656\n",
      "Precision:  0.717\n",
      "Recall:  0.749\n",
      "F1 Score:  0.733\n"
     ]
    }
   ],
   "source": [
    "nn_model = KerasClassifier(build_fn=build_nn, verbose=0)\n",
    "history = nn_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=0)\n",
    "y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int)\n",
    "get_scores(y_test, y_pred_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Hyperparameter Tuning\n",
    "The next step involves optimising the model's hyperparameters to improve its performance.<br><br>\n",
    "Logistic Regression is one of the better performing baseline models, so we can now run that through GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'penalty': 'l1'}\n",
      "Best Score:  0.6266796494644596\n"
     ]
    }
   ],
   "source": [
    "# Choose the best performaing model\n",
    "gs_cv_class = lr_class\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l2','l1'],\n",
    "#     'C': [0.1, 1.0, 10.0],\n",
    "#     'solver': ['liblinear', 'lbfgs'],\n",
    "#     'max_iter': [100],\n",
    "#     'class_weight': [None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gs_cv_class, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "final_model_class = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.616\n",
      "Precision:  0.699\n",
      "Recall:  0.685\n",
      "F1 Score:  0.692\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model_class.predict(X_test)\n",
    "get_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 16, 'dropout': 0.3, 'epochs': 15, 'lr': 0.0005, 'neurons': 32}\n",
      "Best Score: 0.6407010710808179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'neurons': [32, 64, 128],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'lr': [0.001, 0.0005],\n",
    "    'epochs': [15, 30],\n",
    "    'batch_size': [16, 32],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=nn_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "final_model_nn = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n",
      "Precision:  0.741\n",
      "Recall:  0.769\n",
      "F1 Score:  0.755\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model_nn.predict(X_final)\n",
    "get_scores(y_final, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Use Algorithm to predict 25th Feb results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7\n",
      "Precision:  0.741\n",
      "Recall:  0.769\n",
      "F1 Score:  0.755\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = final_model_nn.predict(X_final)\n",
    "y_prob_final = final_model_nn.predict_proba(X_final)\n",
    "get_scores(y_final, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final['pred'] = [x[0] for x in y_pred_final]\n",
    "df_test_final['prob'] = [x[1] for x in y_prob_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "\n",
    "Out of the 130 (260 rows of home/away) matches the model predicted 70% of them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games predicted correctly on final dataset: 0.7\n"
     ]
    }
   ],
   "source": [
    "correct_preds = (df_test_final['pred']==df_test_final['IsWinner']).mean()\n",
    "print(f'Number of games predicted correctly on final dataset: {correct_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
